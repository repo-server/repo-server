# ğŸš€ NeuroServe â€” GPU-Ready FastAPI AI Server

## ğŸ“Š Project Status

| Category      | Badges |
|---------------|--------|
| **Languages** | ![Python](https://img.shields.io/badge/Python-3.12%2B-3776AB?logo=python&logoColor=white) ![HTML5](https://img.shields.io/badge/HTML5-E34F26?logo=html5&logoColor=white) ![CSS3](https://img.shields.io/badge/CSS3-1572B6?logo=css3&logoColor=white) |
| **Framework** | ![FastAPI](https://img.shields.io/badge/FastAPI-0.116.x-009688?logo=fastapi&logoColor=white) |
| **ML / GPU**  | ![PyTorch](https://img.shields.io/badge/PyTorch-2.6.x-EE4C2C?logo=pytorch&logoColor=white) ![CUDA Ready](https://img.shields.io/badge/CUDA-Ready-76B900?logo=nvidia&logoColor=white) |
| **CI**        | [![Ubuntu CI](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-ubuntu.yml/badge.svg)](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-ubuntu.yml) [![Windows CI](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-windows.yml/badge.svg)](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-windows.yml) [![Windows GPU CI](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-gpu.yml/badge.svg)](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-gpu.yml) [![macOS CI](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-macos.yml/badge.svg)](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-macos.yml) |
| **Code Style**| ![Ruff](https://img.shields.io/badge/style-Ruff-000?logo=ruff&logoColor=white) |
| **Tests**     | ![Tests](https://img.shields.io/badge/tests-passing-brightgreen) |
| **Docs**      | [![Docs](https://img.shields.io/badge/docs-available-blue)](docs/API.md) |
| **OS**        | ![Ubuntu](https://img.shields.io/badge/Ubuntu-E95420?logo=ubuntu&logoColor=white) ![Windows](https://img.shields.io/badge/Windows-0078D6?logo=windows&logoColor=white) ![macOS](https://img.shields.io/badge/macOS-000000?logo=apple&logoColor=white) |
| **Version**   | [![GitHub release](https://img.shields.io/github/v/release/TamerOnLine/repo-fastapi)](https://github.com/TamerOnLine/repo-fastapi/releases) |
| **License**   | ![License](https://img.shields.io/badge/License-MIT-green) |
| **Support**   | [![Sponsor](https://img.shields.io/badge/Sponsor-ğŸ’–-pink)](https://paypal.me/tameronline) |
| **GitHub**    | [![Stars](https://img.shields.io/github/stars/TamerOnLine/repo-fastapi?style=social)](https://github.com/TamerOnLine/repo-fastapi/stargazers) [![Forks](https://img.shields.io/github/forks/TamerOnLine/repo-fastapi?style=social)](https://github.co)


---

## ğŸ“– Overview

**NeuroServe** is an **AI Inference Server** built on **FastAPI**, designed to run seamlessly on **GPU (CUDA/ROCm)**, **CPU**, and **macOS MPS**.
It provides ready-to-use REST APIs, a modular **plugin system**, runtime utilities, and a consistent unified response format â€” making it the perfect foundation for AI-powered services.

---

## Quick Setup
 ğŸ”§ Virtualenv quick guide: see **[docs/README_venv.md](docs/README_venv.md)**.

---

## ğŸ“š API Documentation
Detailed API reference and usage examples are available here:
â¡ï¸ [API Documentation](docs/API.md)

---

## âœ¨ Key Features

- ğŸŒ **REST APIs out-of-the-box** with Swagger UI (`/docs`) & ReDoc (`/redoc`).
- âš¡ **PyTorch integration** with automatic device selection (`cuda`, `cpu`, `mps`, `rocm`).
- ğŸ”Œ **Plugin system** to extend functionality with custom AI models or services.
- ğŸ“Š **Runtime tools** for GPU info, warm-up routines, and environment inspection.
- ğŸ§  **Built-in utilities** like a toy model and model size calculator.
- ğŸ§± **Unified JSON responses** for predictable API behavior.
- ğŸ§ª **Cross-platform CI/CD** (Ubuntu, Windows, macOS, Self-hosted GPU).

---

## ğŸ“‚ Project Structure

```text
repo-fastapi/
â”œâ”€ app/                             # application package
â”‚  â”œâ”€ core/                         # settings & configuration
â”‚  â”‚  â””â”€ config.py                  # app settings (Pydantic v2)
â”‚  â”œâ”€ routes/                       # HTTP API routes
â”‚  â”œâ”€ plugins/                      # extensions / integrations
â”‚  â”œâ”€ workflows/                    # workflow definitions & orchestrators
â”‚  â””â”€ templates/                    # Jinja templates (if used)
â”œâ”€ docs/                            # documentation & generated diagrams
â”‚  â”œâ”€ ARCHITECTURE.md               # main architecture report
â”‚  â”œâ”€ architecture.mmd              # Mermaid source (no fences)
â”‚  â”œâ”€ architecture.html             # browser-friendly diagram
â”‚  â”œâ”€ architecture.png              # exported PNG (if mmdc installed)
â”‚  â”œâ”€ runtime.mmd                   # runtime/infra diagram
â”‚  â”œâ”€ imports.mmd                   # Python import graph (if generated)
â”‚  â”œâ”€ endpoints.md                  # discovered API endpoints (if generated)
â”‚  â””â”€ README_venv.md                # virtualenv quick guide
â”œâ”€ tools/                           # project tooling & scripts
â”‚  â””â”€ build_workflows_index.py      # builds docs/workflows-overview.md
â”œâ”€ tests/                           # test suite
â”‚  â””â”€ test_run.py                   # smoke test for app startup
â”œâ”€ gen_arch.py                      # architecture generator script
â”œâ”€ requirements.txt                 # runtime dependencies
â”œâ”€ requirements-dev.txt             # dev dependencies (ruff, pre-commit, pytest, ...)
â”œâ”€ .pre-commit-config.yaml          # pre-commit hooks configuration
â”œâ”€ README.md                        # project overview & usage
â””â”€ LICENSE                          # project license

```

---

## ğŸ—ï¸ Architecture
For a deeper look into the internal design, modules, and flow of the system, see:
â¡ï¸ [Architecture Guide](docs/ARCHITECTURE.md)

---

## âš™ï¸ Installation

### 1. Clone the repository
```bash
git clone https://github.com/USERNAME/gpu-server.git
cd gpu-server
```

### 2. Create a virtual environment
```bash
python -m venv .venv
# Linux/macOS
source .venv/bin/activate
# Windows
.venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. (Optional) Auto-install PyTorch
```bash
python -m scripts.install_torch --gpu    # or --cpu / --rocm
```

---

## ğŸš€ Running the Server

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Available endpoints:
- ğŸ  **Home** â†’ [http://localhost:8000/](http://localhost:8000/)
- â¤ï¸ **Health** â†’ [http://localhost:8000/health](http://localhost:8000/health)
- ğŸ“š **Swagger UI** â†’ [http://localhost:8000/docs](http://localhost:8000/docs)
- ğŸ“˜ **ReDoc** â†’ [http://localhost:8000/redoc](http://localhost:8000/redoc)
- ğŸ§­ **Env Summary** â†’ [http://localhost:8000/env](http://localhost:8000/env)
- ğŸ”Œ **Plugins** â†’ [http://localhost:8000/plugins](http://localhost:8000/plugins)

Quick test:
```bash
curl http://localhost:8000/health
# {"status": "ok"}
```

---

## ğŸ”Œ Plugin System

Each plugin lives in `app/plugins/<name>/` and typically includes:

```
manifest.json
plugin.py        # Defines Plugin class inheriting AIPlugin
README.md        # Documentation
```

API Endpoints:
- `GET /plugins` â€” list all plugins with metadata.
- `POST /plugins/{name}/{task}` â€” execute a task inside a plugin.

Example:
```python
from app.plugins.base import AIPlugin

class Plugin(AIPlugin):
    name = "my_plugin"
    tasks = ["infer"]

    def load(self):
        # Load models/resources once
        ...

    def infer(self, payload: dict) -> dict:
        return {"message": "ok", "payload": payload}
```

---

## Workflow System
A lightweight orchestration layer to chain plugins into reproducible pipelines (steps â†’ plugin + task + payload).
All endpoints are exposed under `/workflow`.

- **Endpoints:** `GET /workflow/ping`, `GET /workflow/presets`, `POST /workflow/run`
- **System Guide (EN):** [app/workflows/README.md](app/workflows/README.md)
- **Workflows Index:** [docs/workflows-overview.md](docs/workflows-overview.md)

---

## ğŸ”„ Available Workflows

A full list of available workflows with their versions, tags, and step counts is maintained in the **Workflows Index**.

â¡ï¸ [View Workflows Index](docs/workflows-overview.md)

---

## ğŸ§© Available Plugins

A full list of available plugins with their providers, tasks, and source files is maintained in the **Plugins Index**.

â¡ï¸ [View Plugins Index](docs/plugins-overview.md)

---

## ğŸ§ª Development

Install dev dependencies:
```bash
pip install -r requirements-dev.txt
pre-commit install
```

Run tests:
```bash
pytest
```

Ruff (lint + format check) runs automatically via pre-commit hooks.

---

## ğŸ§¹ Code Style

We enforce a clean and consistent code style using **Ruff** (linter, import sorter, and formatter).
For full details on configuration, commands, helper scripts, and CI integration, see:

â¡ï¸ [Code Style & Linting Guide](docs/CODE_STYLE_GUIDE.md)

---

## ğŸ“¦ Model Management

Download models in advance:
```bash
python -m scripts.prefetch_models
```

Models are cached in `models_cache/` (see `docs/LICENSES.md` for licenses).

---

## ğŸ­ Deployment Notes

- Use `uvicorn`/`hypercorn` behind a reverse proxy (e.g., Nginx).
- Configure environment with `APP_*` variables instead of hardcoding.
- Enable HTTPS and configure CORS carefully in production.

---

## ğŸ“ Changelog
A complete history of changes and improvements:
â¡ï¸ [CHANGELOG](docs/CHANGELOG.md)

## ğŸ“¦ Release Notes
Details about the initial release v0.1.0:
â¡ï¸ [Release Notes v0.1.0](docs/RELEASE_NOTES_v0.1.0.md)

---

## ğŸ—ºï¸ Roadmap

- [ ] Add `/cuda` endpoint â†’ return detailed CUDA info.
- [ ] Add `/warmup` endpoint for GPU readiness.
- [ ] Provide a **plugin generator CLI**.
- [ ] Implement API Key / JWT authentication.
- [ ] Example plugins: translation, summarization, image classification.
- [ ] Docker support for one-click deployment.
- [ ] Benchmark suite for model inference speed.

---

## ğŸ¤ Contributing

Contributions are welcome!
- Open **Issues** for bugs or ideas.
- Submit **Pull Requests** for improvements.
- Follow style guidelines (Ruff + pre-commit).

---

## ğŸ“œ License
Licensed under the **MIT License** â€” see [LICENSE](./LICENSE).

### ğŸ“œ Model Licenses
Some AI/ML models are licensed separately â€” see [Model Licenses](docs/LICENSES.md).

---
